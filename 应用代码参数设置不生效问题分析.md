### 应用代码参数设置不生效问题分析

#### 问题发现  
用户反馈在应用代码中设置了参数`spark.yarn.maxAppAttempts`为1，应用运行报错结束后还会继续重试一次，未达预期。

`spark.yarn.maxAppAttempts`参数用来控制Spark应用在退出前最多会进行重试的次数。现在平台设置的默认值为2。

相似问题：[Spark应用名在使用yarn-cluster模式提交时不生效](http://support.hwclouds.com/usermanual-mrs/zh-cn_topic_0036027341.html)  

#### 问题跟踪  
首先找到读取maxAppAttempts参数和判断该参数的spark源码，代码位于`ApplicationMaster`类里面：  
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120801.png)  
**代码解读：**通过`getMaxRegAttempts`函数读取`sparkConf`中的`spark.yarn.maxAppAttempts`获取参数。
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120802.png)
**代码解读：**从`sparkConf`中获取到`maxAppAttempts`参数值后，会与yarn端设置的任务最大重试次数作比较。该参数值来自`yarn.resourcemanager.am.max-attempts`，如果此值未设置，取默认值2。spark端设置的`maxAppAttempts`值必须小于等于yarn端的设置值。

参数判断代码：
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120803.png)
**代码解读：**在AM未向RM注销的情况下，只要满足“应用运行成功”和“是最后一次重试”两条件之一，AM才会向RM注销，否则AM不注销，继续重试任务。

看完三段代码，再回看用户遇到的问题，可以推测：应用代码里设置的`.set("spark.yarn.maxAppAttempts", "1")`没有生效，AM读取到的`maxAppAttempts`依然是2。下面来验证推测。

**验证思路**  
既然该参数是从sparkConf参数获取的，那就在spark源码里把该sparkConf的所有值都打印到日志里看下不就行了。验证代码放在读取`maxAppAttempts`之前：
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120804.png)
红框中是验证代码。

然后把编写测试应用，在应用里面设置`.set("spark.yarn.maxAppAttempts", "1")`，在**yarn-cluster**模式下运行应用。
下面是测试应用代码：
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120805.png)

任务运行结束后，在WebUI查看Driver端日志：
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120806.png)
经过搜索，在sparkConf所有的配置中没有发现`spark.yarn.maxAppAttempts`。

同时，经过代码调试发现，在AM中获取到的`maxAppAttempts`值为2。

以上两个证据表明，在AM获取`maxAppAttempts`参数时，应用代码中的`.set("spark.yarn.maxAppAttempts", "1")`设置没有生效。

原因很简单。在AM获取`maxAppAttempts`参数时，应用Driver程序还没有启动，所以应用代码里设置的参数都没有生效。这一点从sparkConf打印的内容里面也可以得到证明。测试代码里设置了`.set("spark.executor.instances", "3")`,但是在sparkConf里面依然是默认值4。

在代码层面，AM获取了`maxAppAttempts`参数后，才运行了`runDriver()`，启动Driver进程：
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120807.png)

启动Driver后，应用代码设置的参数才会生效。为了证明这一点，我在runDriver里也打印了sparkConf内容到日志：
加日志打印代码：
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120808.png)

实际日志内容：
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120809.png)
从实际运行情况来看，`runDriver()`调用后，应用代码设置的参数生效。

#### 问题解决
因为该参数属于系统级变量，不建议在应用级进行设置，如果用户实在有此需求，可以在`spark-submit`任务提交命令里面或者在CBT配置任务参数时添加`--conf spark.yarn.maxAppAttempts=1`。

当然修改spark-defaults.conf文件也可以，但是会影响其他任务，不建议采用此种方式。

#### 问题延伸
那为什么采用追加参数和修改配置文件是可行的呢？因为在spark-submit进程启动时，会把spark-defaults.conf和--conf设置的参数都打在本地文件`__spark__conf__AppID.zip`，然后上传到HDFS，yarn-cluster模式下，AM启动后会从该文件里面获取配置参数。
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120810.png)

我们从HDFS下载该压缩文件，解压看一下里面的内容：
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120811.png)

里面有一个文件`__spark_conf__.properties`，这个文件是submit启动后`yarn.Client`进程生成的，填充的内容就是`spark-submit`命令中`--conf`带的参数和`spark-defaults.conf`里面设置的参数。打开看一下：
![Alt text](https://github.com/alixGuo/Resources/blob/master/2016120812.png)

里面没有应用代码设置的参数，因为这时Driver还没启动。所以说，在yarn-cluster模式下，spark-submit进程启动后，它所能给予AM的参数就是命令行中的--conf和配置文件中参数，Driver代码中的参数设置在spark-submit阶段是获取不到的。

但是在yarn-client模式下，Driver端设置的`maxAppAttempts`参数AM是可以获取到的。因为`spark-submit`判断提交模式，如果是yarn-client模式，就直接提交Driver进程了。这样`yarn.client`进程在打包`__spark_conf__.properties`文件时，sparkConf内容已经被Driver端参数设置更新。AM获取到的就是被Driver更新后的sparkConf。











