## Recording...  

- jvm GC打印时间戳：PrintGCTimeStamps，GC打印日期：PrintGCDateStamps  
- spark2.0+版本默认写hdfs文件的时候是开启压缩的，这样会导致业务直接读HDFS是乱码的，关闭压缩的参数：`set spark.hadoop.mapreduce.output.fileoutputformat.compress=false`  
- Increase this if you get a "buffer limit exceeded" exception inside Kryo：增加`spark.kryoserializer.buffer.max`参数的值，默认64M。    
- SparkStreaming中空batch的处理方法：foreachRDD后判断`rdd.count!=0`或者`!rdd.partitions.isEmpty`    
**参考**：https://www.iteblog.com/archives/1304.html  

- spark.sql.hive.verifyPartitionPath=true ， hive中如有分区数据在hdfs被人为删除的情况，设置该参数  
- 如果executor因为内存打满丢失的情况出现，除了考虑提升executor内存，还需要看executor core的情况，看是不是任务并发度过高。  
- kafka producer发送消息时最后需要将生产者关闭，另外一侧才能消费到：`produce.close`  
- 修改Spark HistoryServer Summary，隐去EventLog Download 一栏后，测试修改无效。后面想到，因为修改的是静态网页，清除浏览器缓存后OK。  
